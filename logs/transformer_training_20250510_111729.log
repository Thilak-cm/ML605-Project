2025-05-10 11:17:29,520 - transformer_training - INFO - Logging initialized. Log file: logs/transformer_training_20250510_111729.log
2025-05-10 11:17:29,520 - transformer_training - INFO - Loading configuration...
2025-05-10 11:17:29,522 - transformer_training - INFO - 
Running hyperparameter optimization...
2025-05-10 11:17:29,522 - transformer_training - INFO - Starting hyperparameter optimization with 50 trials...
2025-05-10 11:17:29,522 - transformer_training - INFO - Each trial will run for up to 50 epochs with early stopping (patience=5)
2025-05-10 11:17:29,522 - transformer_training - INFO - Loading and preprocessing data...
2025-05-10 11:17:36,275 - transformer_training - INFO - Data loaded. Shape: (1215964, 71)
2025-05-10 11:17:36,275 - transformer_training - INFO - Converting hour column to datetime...
2025-05-10 11:17:36,478 - transformer_training - INFO - Scaling features...
2025-05-10 11:17:36,991 - transformer_training - INFO - Data split sizes:
2025-05-10 11:17:36,994 - transformer_training - INFO - Train: 972822 records, 263 zones
2025-05-10 11:17:36,994 - transformer_training - INFO - Val: 121611 records, 260 zones
2025-05-10 11:17:37,006 - transformer_training - INFO - Adjusted timeout per trial: 29184 seconds (based on data size)
2025-05-10 11:17:37,007 - transformer_training - INFO - 
===== Starting Optuna Optimization Loop =====
2025-05-10 11:17:37,008 - transformer_training - INFO - 
========== Starting Trial 0 ==========
2025-05-10 11:17:37,008 - transformer_training - INFO - 
Starting Trial 0
2025-05-10 11:17:37,008 - transformer_training - INFO - Memory usage at start: 960.86 MB
2025-05-10 11:17:37,009 - transformer_training - INFO - Using device: cpu
2025-05-10 11:17:37,012 - transformer_training - INFO - Selected hyperparameters for this trial:
2025-05-10 11:17:37,012 - transformer_training - INFO -   d_model: 24
2025-05-10 11:17:37,012 - transformer_training - INFO -   n_heads: 4
2025-05-10 11:17:37,012 - transformer_training - INFO -   n_encoder_layers: 4
2025-05-10 11:17:37,012 - transformer_training - INFO -   n_decoder_layers: 2
2025-05-10 11:17:37,012 - transformer_training - INFO -   dropout: 0.4697337824270055
2025-05-10 11:17:37,012 - transformer_training - INFO -   learning_rate: 0.002216543539409685
2025-05-10 11:17:37,012 - transformer_training - INFO -   batch_size: 64
2025-05-10 11:17:37,012 - transformer_training - INFO -   input_seq_len: 12
2025-05-10 11:17:37,012 - transformer_training - INFO -   output_seq_len: 12
2025-05-10 11:17:37,012 - transformer_training - INFO - Creating datasets...
2025-05-10 11:26:59,488 - transformer_training - INFO - Created datasets with 966875 training sequences and 116019 validation sequences
2025-05-10 11:26:59,493 - transformer_training - INFO - Initializing model...
2025-05-10 11:26:59,519 - transformer_training - INFO - Starting model training...
