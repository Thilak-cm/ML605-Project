2025-05-10 11:36:08,157 - transformer_training - INFO - Logging initialized. Log file: logs/transformer_training_20250510_113608.log
2025-05-10 11:36:08,157 - transformer_training - INFO - Loading configuration...
2025-05-10 11:36:08,158 - transformer_training - INFO - 
Running hyperparameter optimization...
2025-05-10 11:36:08,158 - transformer_training - INFO - Starting hyperparameter optimization with 30 trials...
2025-05-10 11:36:08,158 - transformer_training - INFO - Each trial will run for up to 20 epochs with early stopping (patience=3)
2025-05-10 11:36:08,158 - transformer_training - INFO - Loading and preprocessing data...
2025-05-10 11:36:13,188 - transformer_training - INFO - Data loaded. Shape: (1215964, 71)
2025-05-10 11:36:13,189 - transformer_training - INFO - Using subset of data for hyperparameter optimization...
2025-05-10 11:36:13,421 - transformer_training - INFO - Reduced data shape: (98464, 71)
2025-05-10 11:36:13,421 - transformer_training - INFO - Converting hour column to datetime...
2025-05-10 11:36:13,451 - transformer_training - INFO - Scaling features...
2025-05-10 11:36:13,490 - transformer_training - INFO - Data split sizes:
2025-05-10 11:36:13,491 - transformer_training - INFO - Train: 78776 records, 50 zones
2025-05-10 11:36:13,491 - transformer_training - INFO - Val: 9849 records, 50 zones
2025-05-10 11:36:13,491 - transformer_training - INFO - Adjusted timeout per trial: 3000 seconds (based on data size)
2025-05-10 11:36:13,492 - transformer_training - INFO - 
===== Starting Optuna Optimization Loop =====
2025-05-10 11:36:13,494 - transformer_training - INFO - 
========== Starting Trial 0 ==========
2025-05-10 11:36:13,494 - transformer_training - INFO - 
Starting Trial 0
2025-05-10 11:36:13,495 - transformer_training - INFO - Memory usage at start: 590.14 MB
2025-05-10 11:36:13,495 - transformer_training - INFO - Using device: cpu
2025-05-10 11:36:13,497 - transformer_training - INFO - Selected hyperparameters for this trial:
2025-05-10 11:36:13,497 - transformer_training - INFO -   d_model: 15
2025-05-10 11:36:13,497 - transformer_training - INFO -   n_heads: 3
2025-05-10 11:36:13,497 - transformer_training - INFO -   n_encoder_layers: 3
2025-05-10 11:36:13,497 - transformer_training - INFO -   n_decoder_layers: 4
2025-05-10 11:36:13,497 - transformer_training - INFO -   dropout: 0.17536511851796585
2025-05-10 11:36:13,497 - transformer_training - INFO -   learning_rate: 0.006552555915834674
2025-05-10 11:36:13,497 - transformer_training - INFO -   batch_size: 64
2025-05-10 11:36:13,497 - transformer_training - INFO -   input_seq_len: 24
2025-05-10 11:36:13,497 - transformer_training - INFO -   output_seq_len: 24
2025-05-10 11:36:13,497 - transformer_training - INFO - Creating datasets...
2025-05-10 11:36:43,812 - transformer_training - INFO - Created datasets with 76426 training sequences and 7499 validation sequences
2025-05-10 11:36:43,814 - transformer_training - INFO - Initializing model...
2025-05-10 11:36:43,833 - transformer_training - INFO - Starting model training...
